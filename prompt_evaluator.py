"""
Prompt æ‰¹é‡è¯„ä¼°å·¥å…·
===================
åŠŸèƒ½ï¼š
1. å¯¼å…¥è¯„ä¼°é›†ï¼ˆExcelï¼‰
2. è‡ªåŠ¨è¯†åˆ« prompt ä¸­çš„ {{å ä½ç¬¦}}ï¼Œæ˜ å°„åˆ°è¯„ä¼°é›†å­—æ®µ
3. æ‰¹é‡æ‹¼æ¥ prompt å¹¶è°ƒç”¨æ¨¡å‹ API
4. è¾“å‡ºç»“æœåˆ° Excel

ä½¿ç”¨æ–¹æ³•ï¼š
    streamlit run prompt_evaluator.py

ä¾èµ–å®‰è£…ï¼š
    pip install streamlit pandas openpyxl openai
"""

import streamlit as st
import pandas as pd
import re
import json
import time
import os
from datetime import datetime
from io import BytesIO
from pathlib import Path
from openai import OpenAI
from concurrent.futures import ThreadPoolExecutor, as_completed


# ============== é…ç½®ç®¡ç† ==============

CONFIG_FILE = Path(__file__).parent / "model_configs.json"

def load_model_configs() -> dict:
    """åŠ è½½æ¨¡å‹é…ç½®"""
    if CONFIG_FILE.exists():
        with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {"models": {}, "default": None}

def save_model_configs(configs: dict):
    """ä¿å­˜æ¨¡å‹é…ç½®"""
    with open(CONFIG_FILE, 'w', encoding='utf-8') as f:
        json.dump(configs, f, ensure_ascii=False, indent=2)

def get_model_list() -> list[str]:
    """è·å–æ‰€æœ‰æ¨¡å‹åç§°åˆ—è¡¨"""
    configs = load_model_configs()
    return list(configs.get("models", {}).keys())

def get_model_config(model_key: str) -> dict:
    """è·å–æŒ‡å®šæ¨¡å‹çš„é…ç½®"""
    configs = load_model_configs()
    return configs.get("models", {}).get(model_key, {})

def add_model_config(name: str, api_base: str, api_key: str, model_name: str, description: str = ""):
    """æ·»åŠ æ–°çš„æ¨¡å‹é…ç½®"""
    configs = load_model_configs()
    configs["models"][name] = {
        "api_base": api_base,
        "api_key": api_key,
        "model_name": model_name,
        "description": description
    }
    if not configs.get("default"):
        configs["default"] = name
    save_model_configs(configs)

def delete_model_config(name: str):
    """åˆ é™¤æ¨¡å‹é…ç½®"""
    configs = load_model_configs()
    if name in configs.get("models", {}):
        del configs["models"][name]
        if configs.get("default") == name:
            configs["default"] = list(configs["models"].keys())[0] if configs["models"] else None
        save_model_configs(configs)


# ============== å·¥å…·å‡½æ•° ==============

def extract_placeholders(prompt_template: str) -> list[str]:
    """ä» prompt æ¨¡æ¿ä¸­æå–æ‰€æœ‰ {{xxx}} å ä½ç¬¦"""
    pattern = r'\{\{([^}]+)\}\}'
    matches = re.findall(pattern, prompt_template)
    # å»é‡å¹¶ä¿æŒé¡ºåº
    seen = set()
    unique = []
    for m in matches:
        if m not in seen:
            seen.add(m)
            unique.append(m)
    return unique


def fill_prompt(template: str, mapping: dict, row: pd.Series) -> str:
    """æ ¹æ®æ˜ å°„å…³ç³»ï¼Œç”¨è¯„ä¼°é›†æ•°æ®å¡«å…… prompt æ¨¡æ¿"""
    filled = template
    for placeholder, excel_field in mapping.items():
        if excel_field and excel_field in row.index:
            value = str(row[excel_field]) if pd.notna(row[excel_field]) else ""
            filled = filled.replace(f"{{{{{placeholder}}}}}", value)
    return filled


def call_llm(client: OpenAI, model: str, prompt: str, system_prompt: str = None, 
             temperature: float = 0.7, max_tokens: int = 2000) -> tuple[str, float]:
    """è°ƒç”¨ LLM APIï¼Œè¿”å› (å“åº”å†…å®¹, è€—æ—¶ç§’æ•°)"""
    messages = []
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})
    messages.append({"role": "user", "content": prompt})
    
    start_time = time.time()
    try:
        response = client.chat.completions.create(
            model=model,
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens
        )
        elapsed = time.time() - start_time
        return response.choices[0].message.content, elapsed
    except Exception as e:
        elapsed = time.time() - start_time
        return f"[ERROR] {str(e)}", elapsed


def batch_evaluate(prompts: list[str], client: OpenAI, model: str, 
                   system_prompt: str, temperature: float, max_tokens: int,
                   max_workers: int = 3, progress_callback=None) -> list[dict]:
    """æ‰¹é‡è°ƒç”¨ LLMï¼Œè¿”å›ç»“æœåˆ—è¡¨"""
    results = [None] * len(prompts)
    
    def process_one(idx: int, prompt: str):
        response, elapsed = call_llm(client, model, prompt, system_prompt, temperature, max_tokens)
        return idx, response, elapsed
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {executor.submit(process_one, i, p): i for i, p in enumerate(prompts)}
        completed = 0
        for future in as_completed(futures):
            idx, response, elapsed = future.result()
            results[idx] = {"response": response, "time": elapsed}
            completed += 1
            if progress_callback:
                progress_callback(completed / len(prompts))
    
    return results


# ============== Streamlit UI ==============

def main():
    st.set_page_config(
        page_title="Prompt æ‰¹é‡è¯„ä¼°å·¥å…·",
        page_icon="ğŸ§ª",
        layout="wide"
    )
    
    st.title("ğŸ§ª Prompt æ‰¹é‡è¯„ä¼°å·¥å…·")
    st.caption("å¯¼å…¥è¯„ä¼°é›† â†’ æ˜ å°„å­—æ®µ â†’ æ‰¹é‡æ¨ç† â†’ å¯¼å‡ºç»“æœ")
    
    # åˆå§‹åŒ– session state
    if "prompts_generated" not in st.session_state:
        st.session_state.prompts_generated = []
    if "results" not in st.session_state:
        st.session_state.results = []
    
    # ===== ä¾§è¾¹æ ï¼šAPI é…ç½® =====
    with st.sidebar:
        st.header("âš™ï¸ æ¨¡å‹é€‰æ‹©")
        
        # åŠ è½½é…ç½®
        configs = load_model_configs()
        model_list = get_model_list()
        
        if model_list:
            # ä¸‹æ‹‰é€‰æ‹©æ¨¡å‹
            default_idx = 0
            if configs.get("default") in model_list:
                default_idx = model_list.index(configs["default"])
            
            selected_model_key = st.selectbox(
                "é€‰æ‹©æ¨¡å‹",
                model_list,
                index=default_idx,
                help="ä»å·²é…ç½®çš„æ¨¡å‹ä¸­é€‰æ‹©"
            )
            
            # è·å–é€‰ä¸­æ¨¡å‹çš„é…ç½®
            model_config = get_model_config(selected_model_key)
            api_base = model_config.get("api_base", "")
            api_key = model_config.get("api_key", "")
            model_name = model_config.get("model_name", "")
            
            # æ˜¾ç¤ºå½“å‰é…ç½®ä¿¡æ¯
            with st.expander("ğŸ“‹ å½“å‰æ¨¡å‹é…ç½®", expanded=False):
                st.text(f"API Base: {api_base}")
                st.text(f"æ¨¡å‹: {model_name}")
                if model_config.get("description"):
                    st.caption(model_config["description"])
        else:
            st.warning("æš‚æ— æ¨¡å‹é…ç½®ï¼Œè¯·æ·»åŠ ")
            api_base = ""
            api_key = ""
            model_name = ""
        
        st.divider()
        
        # æ·»åŠ /ç®¡ç†æ¨¡å‹é…ç½®
        with st.expander("â• æ·»åŠ æ–°æ¨¡å‹", expanded=not model_list):
            new_name = st.text_input("é…ç½®åç§°", placeholder="å¦‚: GPT-4oã€Claude-3.5")
            new_api_base = st.text_input("API Base URL", placeholder="https://api.openai.com/v1")
            new_api_key = st.text_input("API Key", type="password", placeholder="sk-xxx")
            new_model_name = st.text_input("æ¨¡å‹åç§°", placeholder="gpt-4o")
            new_description = st.text_input("å¤‡æ³¨ (å¯é€‰)", placeholder="ç”¨é€”æè¿°")
            
            if st.button("ğŸ’¾ ä¿å­˜é…ç½®", use_container_width=True):
                if new_name and new_api_base and new_api_key and new_model_name:
                    add_model_config(new_name, new_api_base, new_api_key, new_model_name, new_description)
                    st.success(f"âœ… å·²æ·»åŠ  {new_name}")
                    st.rerun()
                else:
                    st.error("è¯·å¡«å†™å®Œæ•´ä¿¡æ¯")
        
        # åˆ é™¤æ¨¡å‹
        if model_list:
            with st.expander("ğŸ—‘ï¸ åˆ é™¤æ¨¡å‹"):
                delete_target = st.selectbox("é€‰æ‹©è¦åˆ é™¤çš„æ¨¡å‹", model_list, key="delete_select")
                if st.button("ç¡®è®¤åˆ é™¤", type="secondary"):
                    delete_model_config(delete_target)
                    st.success(f"å·²åˆ é™¤ {delete_target}")
                    st.rerun()
        
        st.divider()
        st.subheader("æ¨ç†å‚æ•°")
        temperature = st.slider("Temperature", 0.0, 2.0, 0.7, 0.1)
        max_tokens = st.number_input("Max Tokens", 100, 8000, 2000, 100)
        max_workers = st.slider("å¹¶å‘æ•°", 1, 10, 3, help="åŒæ—¶è°ƒç”¨ API çš„çº¿ç¨‹æ•°")
    
    # ===== ä¸»ç•Œé¢ =====
    tab1, tab2, tab3 = st.tabs(["ğŸ“ Step 1: é…ç½® Prompt", "ğŸ”— Step 2: æ˜ å°„å­—æ®µ", "ğŸš€ Step 3: æ‰¹é‡è¯„ä¼°"])
    
    # ----- Tab 1: Prompt é…ç½® -----
    with tab1:
        col1, col2 = st.columns([1, 1])
        
        with col1:
            st.subheader("System Prompt (å¯é€‰)")
            system_prompt = st.text_area(
                "ç³»ç»Ÿæç¤ºè¯",
                height=150,
                placeholder="è¾“å…¥ç³»ç»Ÿçº§æŒ‡ä»¤ï¼Œå¦‚è§’è‰²è®¾å®šç­‰...",
                key="system_prompt"
            )
        
        with col2:
            st.subheader("User Prompt æ¨¡æ¿")
            prompt_template = st.text_area(
                "ç”¨æˆ·æç¤ºè¯æ¨¡æ¿",
                height=150,
                placeholder="ä½¿ç”¨ {{å­—æ®µå}} ä½œä¸ºå ä½ç¬¦ï¼Œå¦‚ï¼š\n\nåˆ†æä»¥ä¸‹èŠå¤©è®°å½•ï¼š\n{{chat_input}}\n\nç”¨æˆ·èƒŒæ™¯ï¼š{{user_info}}",
                key="prompt_template"
            )
        
        # è¯†åˆ«å ä½ç¬¦
        if prompt_template:
            placeholders = extract_placeholders(prompt_template)
            if placeholders:
                st.success(f"âœ… è¯†åˆ«åˆ° {len(placeholders)} ä¸ªå ä½ç¬¦ï¼š`{'`, `'.join(placeholders)}`")
            else:
                st.warning("âš ï¸ æœªè¯†åˆ«åˆ° {{}} æ ¼å¼çš„å ä½ç¬¦")
    
    # ----- Tab 2: å­—æ®µæ˜ å°„ -----
    with tab2:
        st.subheader("ä¸Šä¼ è¯„ä¼°é›†")
        uploaded_file = st.file_uploader(
            "é€‰æ‹©è¯„ä¼°é›†æ–‡ä»¶",
            type=["xlsx", "xls", "csv"],
            help="æ”¯æŒ Excel (.xlsx/.xls) å’Œ CSV (.csv) æ ¼å¼"
        )
        
        if uploaded_file:
            try:
                # æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©è¯»å–æ–¹å¼
                file_name = uploaded_file.name.lower()
                if file_name.endswith('.csv'):
                    df = pd.read_csv(uploaded_file)
                else:
                    df = pd.read_excel(uploaded_file)
                st.success(f"âœ… æˆåŠŸåŠ è½½ {len(df)} æ¡æ•°æ®")
                
                # é¢„è§ˆæ•°æ®
                with st.expander("ğŸ“Š é¢„è§ˆè¯„ä¼°é›†æ•°æ®", expanded=True):
                    st.dataframe(df.head(10), use_container_width=True)
                
                # å­—æ®µæ˜ å°„
                prompt_template = st.session_state.get("prompt_template", "")
                placeholders = extract_placeholders(prompt_template)
                
                if placeholders:
                    st.subheader("ğŸ”— å­—æ®µæ˜ å°„")
                    st.caption("å°† Prompt ä¸­çš„å ä½ç¬¦æ˜ å°„åˆ°è¯„ä¼°é›†çš„å­—æ®µ")
                    
                    excel_columns = ["(ä¸æ˜ å°„)"] + list(df.columns)
                    mapping = {}
                    
                    cols = st.columns(min(len(placeholders), 3))
                    for i, placeholder in enumerate(placeholders):
                        with cols[i % 3]:
                            selected = st.selectbox(
                                f"`{{{{{placeholder}}}}}`",
                                excel_columns,
                                key=f"map_{placeholder}"
                            )
                            if selected != "(ä¸æ˜ å°„)":
                                mapping[placeholder] = selected
                    
                    # ä¿å­˜æ˜ å°„åˆ° session
                    st.session_state.mapping = mapping
                    st.session_state.df = df
                    
                    # é¢„è§ˆæ‹¼æ¥åçš„ prompt
                    if mapping and len(df) > 0:
                        st.subheader("ğŸ‘€ é¢„è§ˆæ‹¼æ¥æ•ˆæœ")
                        preview_idx = st.slider("é€‰æ‹©é¢„è§ˆè¡Œ", 0, len(df)-1, 0)
                        preview_prompt = fill_prompt(prompt_template, mapping, df.iloc[preview_idx])
                        st.text_area("æ‹¼æ¥åçš„ Prompt", preview_prompt, height=200, disabled=True)
                else:
                    st.warning("âš ï¸ è¯·å…ˆåœ¨ Step 1 ä¸­è®¾ç½®åŒ…å« {{}} å ä½ç¬¦çš„ Prompt æ¨¡æ¿")
            
            except Exception as e:
                st.error(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥ï¼š{e}")
    
    # ----- Tab 3: æ‰¹é‡è¯„ä¼° -----
    with tab3:
        st.subheader("æ‰¹é‡æ¨ç†")
        
        # æ£€æŸ¥å‰ç½®æ¡ä»¶
        ready = True
        if not api_key or not api_base:
            st.warning("âš ï¸ è¯·åœ¨ä¾§è¾¹æ é€‰æ‹©æˆ–æ·»åŠ æ¨¡å‹é…ç½®")
            ready = False
        if not st.session_state.get("prompt_template"):
            st.warning("âš ï¸ è¯·åœ¨ Step 1 é…ç½® Prompt æ¨¡æ¿")
            ready = False
        if "df" not in st.session_state:
            st.warning("âš ï¸ è¯·åœ¨ Step 2 ä¸Šä¼ è¯„ä¼°é›†")
            ready = False
        
        if ready:
            df = st.session_state.df
            mapping = st.session_state.get("mapping", {})
            prompt_template = st.session_state.prompt_template
            system_prompt = st.session_state.get("system_prompt", "")
            
            st.info(f"ğŸ“Š è¯„ä¼°é›†å…± {len(df)} æ¡æ•°æ®ï¼Œå·²æ˜ å°„ {len(mapping)} ä¸ªå­—æ®µ")
            st.success(f"ğŸ¤– å½“å‰æ¨¡å‹: **{model_name}** ({api_base})")
            
            # é€‰æ‹©è¯„ä¼°èŒƒå›´
            col1, col2 = st.columns(2)
            with col1:
                start_idx = st.number_input("èµ·å§‹è¡Œ", 0, len(df)-1, 0)
            with col2:
                end_idx = st.number_input("ç»“æŸè¡Œ", start_idx, len(df)-1, min(start_idx + 9, len(df)-1))
            
            eval_count = end_idx - start_idx + 1
            st.caption(f"å°†è¯„ä¼°ç¬¬ {start_idx} åˆ° {end_idx} è¡Œï¼Œå…± {eval_count} æ¡")
            
            # å¼€å§‹è¯„ä¼°æŒ‰é’®
            if st.button("ğŸš€ å¼€å§‹æ‰¹é‡è¯„ä¼°", type="primary", use_container_width=True):
                # ç”Ÿæˆæ‰€æœ‰ prompt
                prompts = []
                for i in range(start_idx, end_idx + 1):
                    filled = fill_prompt(prompt_template, mapping, df.iloc[i])
                    prompts.append(filled)
                
                st.session_state.prompts_generated = prompts
                
                # è°ƒç”¨ API
                client = OpenAI(base_url=api_base, api_key=api_key)
                
                progress_bar = st.progress(0, text="æ­£åœ¨è¯„ä¼°...")
                
                def update_progress(p):
                    progress_bar.progress(p, text=f"è¿›åº¦: {int(p*100)}%")
                
                with st.spinner("æ­£åœ¨æ‰¹é‡è°ƒç”¨ API..."):
                    results = batch_evaluate(
                        prompts, client, model_name,
                        system_prompt if system_prompt else None,
                        temperature, max_tokens, max_workers,
                        progress_callback=update_progress
                    )
                
                st.session_state.results = results
                progress_bar.progress(1.0, text="âœ… å®Œæˆ!")
                st.success(f"âœ… è¯„ä¼°å®Œæˆï¼å…±å¤„ç† {len(results)} æ¡")
        
        # å±•ç¤ºç»“æœ
        if st.session_state.results:
            st.subheader("ğŸ“‹ è¯„ä¼°ç»“æœ")
            
            results = st.session_state.results
            df = st.session_state.df
            start_idx = int(st.session_state.get("start_idx", 0))
            
            # æ„å»ºç»“æœ DataFrame
            result_data = []
            for i, res in enumerate(results):
                row_idx = start_idx + i
                row_data = {
                    "åºå·": row_idx,
                    "æ¨¡å‹å“åº”": res["response"],
                    "è€—æ—¶(ç§’)": round(res["time"], 2)
                }
                # æ·»åŠ åŸå§‹æ•°æ®å­—æ®µ
                if row_idx < len(df):
                    for col in df.columns:
                        row_data[f"[åŸ]{col}"] = df.iloc[row_idx][col]
                result_data.append(row_data)
            
            result_df = pd.DataFrame(result_data)
            
            # å±•ç¤ºç»“æœè¡¨æ ¼
            st.dataframe(result_df, use_container_width=True, height=400)
            
            # å¯¼å‡ºç»“æœ
            st.subheader("ğŸ’¾ å¯¼å‡ºç»“æœ")
            
            # ç”Ÿæˆ Excel
            output = BytesIO()
            with pd.ExcelWriter(output, engine='openpyxl') as writer:
                result_df.to_excel(writer, index=False, sheet_name='è¯„ä¼°ç»“æœ')
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            st.download_button(
                label="ğŸ“¥ ä¸‹è½½è¯„ä¼°ç»“æœ (Excel)",
                data=output.getvalue(),
                file_name=f"eval_results_{timestamp}.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                type="primary"
            )
            
            # è¯¦ç»†æŸ¥çœ‹å•æ¡ç»“æœ
            st.subheader("ğŸ” è¯¦ç»†æŸ¥çœ‹")
            view_idx = st.selectbox(
                "é€‰æ‹©æŸ¥çœ‹çš„ç»“æœ",
                range(len(results)),
                format_func=lambda x: f"ç¬¬ {start_idx + x} è¡Œ"
            )
            
            col1, col2 = st.columns(2)
            with col1:
                st.markdown("**è¾“å…¥ Prompt:**")
                st.text_area(
                    "prompt",
                    st.session_state.prompts_generated[view_idx],
                    height=300,
                    disabled=True,
                    label_visibility="collapsed"
                )
            with col2:
                st.markdown("**æ¨¡å‹å“åº”:**")
                st.text_area(
                    "response",
                    results[view_idx]["response"],
                    height=300,
                    disabled=True,
                    label_visibility="collapsed"
                )


if __name__ == "__main__":
    main()
